{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from time import time\n",
    "from functools import reduce\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import avg, col, concat, desc, explode, lit, min, max, split, udf, isnan, when, rank, from_unixtime\n",
    "from pyspark.sql.types import IntegerType, DateType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[artist: string, auth: string, firstName: string, gender: string, itemInSession: bigint, lastName: string, length: double, level: string, location: string, method: string, page: string, registration: bigint, sessionId: bigint, song: string, status: bigint, ts: bigint, userAgent: string, userId: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'mini_sparkify_event_data.json'\n",
    "df = spark.read.json(file_path)\n",
    "df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count() #number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns) #number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------+---------+------+------------------+--------+-----------------+------+-----------------+------+-------+--------------------+-----------------+--------------------+------------------+--------------------+--------------------+-----------------+\n",
      "|summary|            artist|      auth|firstName|gender|     itemInSession|lastName|           length| level|         location|method|   page|        registration|        sessionId|                song|            status|                  ts|           userAgent|           userId|\n",
      "+-------+------------------+----------+---------+------+------------------+--------+-----------------+------+-----------------+------+-------+--------------------+-----------------+--------------------+------------------+--------------------+--------------------+-----------------+\n",
      "|  count|            228108|    286500|   278154|278154|            286500|  278154|           228108|286500|           278154|286500| 286500|              278154|           286500|              228108|            286500|              286500|              278154|           286500|\n",
      "|   mean| 551.0852017937219|      null|     null|  null|114.41421291448516|    null|249.1171819778458|  null|             null|  null|   null|1.535358834084427...|1041.526554973822|            Infinity|210.05459685863875|1.540956889810483...|                null|59682.02278593872|\n",
      "| stddev|1217.7693079161374|      null|     null|  null|129.76726201140994|    null|99.23517921058361|  null|             null|  null|   null| 3.291321616327586E9|726.7762634630741|                 NaN| 31.50507848842214|1.5075439608226302E9|                null|109091.9499991047|\n",
      "|    min|               !!!| Cancelled| Adelaida|     F|                 0|   Adams|          0.78322|  free|       Albany, OR|   GET|  About|       1521380675000|                1|\u001c",
      "ÃÂg ÃÂtti Gr...|               200|       1538352117000|\"Mozilla/5.0 (Mac...|                 |\n",
      "|    max| ÃÂlafur Arnalds|Logged Out|   Zyonna|     M|              1321|  Wright|       3024.66567|  paid|Winston-Salem, NC|   PUT|Upgrade|       1543247354000|             2474|ÃÂau hafa slopp...|               404|       1543799476000|Mozilla/5.0 (comp...|               99|\n",
      "+-------+------------------+----------+---------+------+------------------+--------+-----------------+------+-----------------+------+-------+--------------------+-----------------+--------------------+------------------+--------------------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values\n",
      "\n",
      "artist : 58392\n",
      "firstName : 8346\n",
      "gender : 8346\n",
      "lastName : 8346\n",
      "length : 58392\n",
      "location : 8346\n",
      "registration : 8346\n",
      "song : 58392\n",
      "userAgent : 8346\n",
      "userId : 8346\n"
     ]
    }
   ],
   "source": [
    "#missing values\n",
    "def get_nulls_count(df):\n",
    "    \"\"\"\n",
    "    Checks for the null values for each column and returns the count of null values.\n",
    "    \n",
    "    Arguments :\n",
    "    df : spark dataframe\n",
    "    \n",
    "    Returns:\n",
    "    None. print mising values count for each column\n",
    "    \"\"\"\n",
    "    print('Missing Values\\n')\n",
    "    \n",
    "    for col in df.columns:\n",
    "        null_count = df.filter((isnan(df[col])) | (df[col].isNull()) | (df[col] == '')).count()\n",
    "        if null_count > 0:\n",
    "            print(f\"{col} : {null_count}\")\n",
    "            \n",
    "get_nulls_count(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are so many null counts equal to 8346 so it is obvious there is a pattern. Let's look at the userId And some have 58392 null value counts which are related to song info such as song title, artist, length of the song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "|      |\n",
      "|    10|\n",
      "|   100|\n",
      "|100001|\n",
      "|100002|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"userId\").dropDuplicates().sort(\"userId\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User ids have empty strings let's see those user behavior to get the sense of who are those users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which page did user id \"\" (empty string) visit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|        blank_pages|\n",
      "+-------------------+\n",
      "|               Home|\n",
      "|              About|\n",
      "|Submit Registration|\n",
      "|              Login|\n",
      "|           Register|\n",
      "|               Help|\n",
      "|              Error|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.userId == '') \\\n",
    "    .select(col('page') \\\n",
    "    .alias('blank_pages')) \\\n",
    "    .dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which page did user id \"\" (empty string) NOT visit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roll Advert\n",
      "Settings\n",
      "Logout\n",
      "Cancel\n",
      "Add Friend\n",
      "NextSong\n",
      "Cancellation Confirmation\n",
      "Thumbs Down\n",
      "Save Settings\n",
      "Downgrade\n",
      "Submit Upgrade\n",
      "Add to Playlist\n",
      "Thumbs Up\n",
      "Submit Downgrade\n",
      "Upgrade\n"
     ]
    }
   ],
   "source": [
    "# filter for users with blank user id\n",
    "blank_pages = df.filter(df.userId == '') \\\n",
    "    .select(col('page') \\\n",
    "    .alias('blank_pages')) \\\n",
    "    .dropDuplicates()\n",
    "\n",
    "# get a list of possible pages that could be visited\n",
    "all_pages = df.select('page').dropDuplicates()\n",
    "\n",
    "# find values in all_pages that are not in blank_pages\n",
    "# these are the pages that the blank user did not go to\n",
    "for row in set(all_pages.collect()) - set(blank_pages.collect()):\n",
    "    print(row.page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps it represents users who have not signed up yet or who are signed out and are about to log in. So we can remove these users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('userId').distinct().count() #Unique Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.dropna(how = \"any\", subset = [\"userId\", \"sessionId\"])\n",
    "df_clean = df_clean.filter(df[\"userId\"] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.select('userId').distinct().count() #Unique Users after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278154"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.count() #number of rows after cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data type issues and column value issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[artist: string, auth: string, firstName: string, gender: string, itemInSession: bigint, lastName: string, length: double, level: string, location: string, method: string, page: string, registration: bigint, sessionId: bigint, song: string, status: bigint, ts: bigint, userAgent: string, userId: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Registration and ts(time stamp) are in bigint format instead of date format. Let's transform these two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "| registration|           ts|\n",
      "+-------------+-------------+\n",
      "|1538173362000|1538352117000|\n",
      "|1538331630000|1538352180000|\n",
      "|1538173362000|1538352394000|\n",
      "|1538331630000|1538352416000|\n",
      "|1538173362000|1538352676000|\n",
      "+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.select('registration','ts').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|   registrationTime|          eventTime|\n",
      "+-------------------+-------------------+\n",
      "|2018-09-28 22:22:42|2018-10-01 00:01:57|\n",
      "|2018-09-30 18:20:30|2018-10-01 00:03:00|\n",
      "|2018-09-28 22:22:42|2018-10-01 00:06:34|\n",
      "|2018-09-30 18:20:30|2018-10-01 00:06:56|\n",
      "|2018-09-28 22:22:42|2018-10-01 00:11:16|\n",
      "+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#transforming ts and registration\n",
    "get_date = udf(lambda x : datetime.datetime.fromtimestamp(x / 1000.0).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "df_clean = df_clean.withColumn('eventTime' , get_date('ts'))\n",
    "df_clean = df_clean.withColumn('registrationTime' , get_date('registration'))\n",
    "df_clean.select('registrationTime','eventTime').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              artist|\n",
      "+--------------------+\n",
      "|      The Black Keys|\n",
      "|         Silverstein|\n",
      "|           Kate Nash|\n",
      "|        Yann Tiersen|\n",
      "|    Jane's Addiction|\n",
      "|          Tim Hughes|\n",
      "|          Carl Craig|\n",
      "|Dashboard Confess...|\n",
      "|Yonder Mountain S...|\n",
      "|           Los Lobos|\n",
      "|Pete Rock & C.L. ...|\n",
      "|        Ziggy Marley|\n",
      "|      Jarabe De Palo|\n",
      "|               Rufio|\n",
      "|WC And The Maad C...|\n",
      "|      Jorge Gonzalez|\n",
      "|                Silk|\n",
      "|  The Watts Prophets|\n",
      "|            La Shica|\n",
      "|        Generation X|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+\n",
      "|     auth|\n",
      "+---------+\n",
      "|Cancelled|\n",
      "|Logged In|\n",
      "+---------+\n",
      "\n",
      "+----------+\n",
      "| firstName|\n",
      "+----------+\n",
      "|    Maddox|\n",
      "|    Karter|\n",
      "|     Lucas|\n",
      "|     Grace|\n",
      "|  Antonina|\n",
      "|   Lorelei|\n",
      "|   Adriana|\n",
      "|  Isabella|\n",
      "|     James|\n",
      "|    Brooke|\n",
      "|  Benjamin|\n",
      "|  Giovanni|\n",
      "|     Davis|\n",
      "|    Nathan|\n",
      "|   Antonio|\n",
      "| Christian|\n",
      "|     Brisa|\n",
      "|Alexandria|\n",
      "|   Spencer|\n",
      "|    Jordan|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|     F|\n",
      "|     M|\n",
      "+------+\n",
      "\n",
      "+-------------+\n",
      "|itemInSession|\n",
      "+-------------+\n",
      "|           26|\n",
      "|           29|\n",
      "|          474|\n",
      "|          964|\n",
      "|           65|\n",
      "|          191|\n",
      "|          418|\n",
      "|          541|\n",
      "|          558|\n",
      "|         1010|\n",
      "|         1224|\n",
      "|         1258|\n",
      "|         1277|\n",
      "|          222|\n",
      "|          270|\n",
      "|          293|\n",
      "|          730|\n",
      "|          938|\n",
      "|         1127|\n",
      "|         1145|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+\n",
      "|lastName|\n",
      "+--------+\n",
      "|Harrison|\n",
      "|Thornton|\n",
      "|    Pena|\n",
      "|   Jones|\n",
      "|  Santos|\n",
      "|   Knapp|\n",
      "|  Walton|\n",
      "| Sanchez|\n",
      "|Franklin|\n",
      "|  Henson|\n",
      "| Aguilar|\n",
      "| Schmidt|\n",
      "|  Nguyen|\n",
      "|Calderon|\n",
      "|  Farley|\n",
      "|   Floyd|\n",
      "|    Wolf|\n",
      "|Phillips|\n",
      "|  Barnes|\n",
      "| Spencer|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+\n",
      "|   length|\n",
      "+---------+\n",
      "|200.75057|\n",
      "|317.30893|\n",
      "|260.28363|\n",
      "|241.94567|\n",
      "|262.79138|\n",
      "|216.47628|\n",
      "|361.66485|\n",
      "|231.94077|\n",
      "|375.03955|\n",
      "|213.75955|\n",
      "|429.16526|\n",
      "|381.77914|\n",
      "|347.81995|\n",
      "|227.83955|\n",
      "|356.20526|\n",
      "|401.76281|\n",
      "|387.16036|\n",
      "|216.97261|\n",
      "|170.97098|\n",
      "|174.23628|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+\n",
      "|level|\n",
      "+-----+\n",
      "| free|\n",
      "| paid|\n",
      "+-----+\n",
      "\n",
      "+--------------------+\n",
      "|            location|\n",
      "+--------------------+\n",
      "|     Gainesville, FL|\n",
      "|Atlantic City-Ham...|\n",
      "|Deltona-Daytona B...|\n",
      "|San Diego-Carlsba...|\n",
      "|Cleveland-Elyria, OH|\n",
      "|Kingsport-Bristol...|\n",
      "|New Haven-Milford...|\n",
      "|Birmingham-Hoover...|\n",
      "|  Corpus Christi, TX|\n",
      "|         Dubuque, IA|\n",
      "|Las Vegas-Henders...|\n",
      "|Indianapolis-Carm...|\n",
      "|Seattle-Tacoma-Be...|\n",
      "|          Albany, OR|\n",
      "|   Winston-Salem, NC|\n",
      "|     Bakersfield, CA|\n",
      "|Los Angeles-Long ...|\n",
      "|Minneapolis-St. P...|\n",
      "|San Francisco-Oak...|\n",
      "|Phoenix-Mesa-Scot...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+\n",
      "|method|\n",
      "+------+\n",
      "|   PUT|\n",
      "|   GET|\n",
      "+------+\n",
      "\n",
      "+--------------------+\n",
      "|                page|\n",
      "+--------------------+\n",
      "|              Cancel|\n",
      "|    Submit Downgrade|\n",
      "|         Thumbs Down|\n",
      "|                Home|\n",
      "|           Downgrade|\n",
      "|         Roll Advert|\n",
      "|              Logout|\n",
      "|       Save Settings|\n",
      "|Cancellation Conf...|\n",
      "|               About|\n",
      "|            Settings|\n",
      "|     Add to Playlist|\n",
      "|          Add Friend|\n",
      "|            NextSong|\n",
      "|           Thumbs Up|\n",
      "|                Help|\n",
      "|             Upgrade|\n",
      "|               Error|\n",
      "|      Submit Upgrade|\n",
      "+--------------------+\n",
      "\n",
      "+-------------+\n",
      "| registration|\n",
      "+-------------+\n",
      "|1529027541000|\n",
      "|1533192032000|\n",
      "|1537779419000|\n",
      "|1534627466000|\n",
      "|1535389443000|\n",
      "|1536642109000|\n",
      "|1537167593000|\n",
      "|1536854322000|\n",
      "|1533908361000|\n",
      "|1536817381000|\n",
      "|1536663902000|\n",
      "|1537672236000|\n",
      "|1536956945000|\n",
      "|1537611935000|\n",
      "|1537751138000|\n",
      "|1533532298000|\n",
      "|1537057938000|\n",
      "|1537964483000|\n",
      "|1528772084000|\n",
      "|1537440271000|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+\n",
      "|sessionId|\n",
      "+---------+\n",
      "|       29|\n",
      "|       26|\n",
      "|      474|\n",
      "|      964|\n",
      "|     1697|\n",
      "|     1806|\n",
      "|     2040|\n",
      "|     1950|\n",
      "|     2214|\n",
      "|      418|\n",
      "|       65|\n",
      "|      541|\n",
      "|      558|\n",
      "|     1010|\n",
      "|     1224|\n",
      "|     1277|\n",
      "|     1258|\n",
      "|     1360|\n",
      "|     1840|\n",
      "|     2173|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+\n",
      "|                song|\n",
      "+--------------------+\n",
      "|Never Gonna Be Al...|\n",
      "|         TULENLIEKKI|\n",
      "|      Underwaterboys|\n",
      "|Saor (Free)/News ...|\n",
      "|               New E|\n",
      "|The Geeks Were Right|\n",
      "|              Heaven|\n",
      "|      Call Me Manana|\n",
      "|             Push It|\n",
      "|Cool Monsoon (Wea...|\n",
      "|The Fun Lovin' Cr...|\n",
      "|Turn Your Lights ...|\n",
      "|          Growing Up|\n",
      "|          Positivity|\n",
      "|         Miracle Man|\n",
      "|        I'm The Drug|\n",
      "|I've Just Seen A ...|\n",
      "|            Anna May|\n",
      "|All The Things Sh...|\n",
      "|    Blue Suede Shoes|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get unique values of all columns / features to investigate the column values\n",
    "for col in df_clean.columns:\n",
    "    df_clean.select([col]).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Location has city name and state name we can extract the state name for better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting state\n",
    "get_state = udf(lambda x : x[-2:])\n",
    "df_clean = df_clean.withColumn('locationState' , get_state('location'))\n",
    "df_clean.select('locationState').distinct().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time range in the data\n",
    "max_date = df_clean.select('eventTime').agg({'eventTime': 'max'}).collect()[0][0]\n",
    "min_date = df_clean.select('eventTime').agg({'eventTime': 'min'}).collect()[0][0]\n",
    "range_date = f'min : {min_date} , max : {max_date}'\n",
    "print(range_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, We have 2 months of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distinct page events\n",
    "df_clean.select('page').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create feature churn\n",
    "churn_func = udf(lambda x : 1 if x == 'Cancellation Confirmation' else 0, IntegerType())\n",
    "\n",
    "df_clean = df_clean.withColumn('churn_event', churn_func('page'))\n",
    "df_clean = df_clean.withColumn('churn', max('churn_event').over(Window.partitionBy('userId')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_count = df_clean.dropDuplicates(['userId', 'churn']).groupby(['churn']).count().toPandas()\n",
    "\n",
    "sns.barplot(x='churn', y='count', data=churn_count);\n",
    "plt.xlabel('churn')\n",
    "plt.ylabel('count')\n",
    "plt.title('How many user churned?');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of registered days between churned and non churned users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the registered_days for churned and not churned users\n",
    "registered_days = df_clean.select('userId','churn','registration','ts')\n",
    "registered_days = registered_days.withColumn('registeredDays',(df_clean.ts - df_clean.registration)/(1000*3600*24))\\\n",
    "                  .groupBy('userId','churn').agg({'registeredDays': 'max'}).toPandas()\n",
    "\n",
    "registered_days.rename(columns= {'max(registeredDays)' : 'registeredDays'},inplace = True)\n",
    "registered_days.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data = registered_days , x = 'churn', y = 'registeredDays')\n",
    "plt.xlabel('Churn')\n",
    "plt.ylabel('Registered days')\n",
    "plt.title('Distribution of registered days of churned and non-churned users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(registered_days.groupby('churn')['registeredDays'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a difference between distribution of registered days between two groups. 50% of the churned users left the service within first 50 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How long do users listen to the songs on average in single session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_session_length = df_clean.select('userId','churn','sessionId','length').groupBy('userId','churn','sessionId').agg({'length' : 'mean'})\\\n",
    "        .groupBy('userId','churn').agg({'avg(length)' : 'mean'}).toPandas()\n",
    "\n",
    "avg_session_length.rename(columns= {'avg(avg(length))' : 'avgSessionLength'},inplace = True)\n",
    "avg_session_length.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data = avg_session_length , x = 'churn', y = 'avgSessionLength')\n",
    "plt.xlabel('Churn')\n",
    "plt.ylabel('Average session length')\n",
    "plt.title('Distribution of Average session length of churned and non-churned users');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Is there any difference in the Location?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locations (state) of the users\n",
    "location_state = df_clean.dropDuplicates(['userId', 'locationState']).groupby('locationState').agg({'churn' : 'mean'}).toPandas()\n",
    "location_state.rename(columns= {'avg(churn)' : 'churnProportion'},inplace = True)\n",
    "location_state.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 6))\n",
    "\n",
    "sns.barplot(x='locationState', y='churnProportion', data=location_state, color = sns.color_palette()[0]);\n",
    "plt.xlabel('location (state)')\n",
    "plt.ylabel('Churn Proportion')\n",
    "plt.title('Churn proportion in each state');\n",
    "plt.legend(title='churn', loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of number of unique sessions for churn and non churn users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sessions = df_clean.select('userId', 'churn', 'sessionId').drop_duplicates().groupBy(['userId', 'churn'])\\\n",
    "                  .count().toPandas()\n",
    " \n",
    "# compare two groups of users\n",
    "g = sns.FacetGrid(unique_sessions, col=\"churn\", sharey=False)\n",
    "g.map(plt.hist, \"count\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of unique sessions per user is less for churn users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender of the user\n",
    "gender_df = df_clean.select(['userId', 'gender']).dropDuplicates(['userId']).replace(['F', 'M'], ['0', '1'], 'gender')\n",
    "gender_df = gender_df.withColumn('gender', gender_df.gender.cast('int'))\n",
    "gender_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# level of the user\n",
    "payment_df = df_clean.select(['userId', 'level']).dropDuplicates(['userId']).replace(['paid', 'free'], ['0', '1'], 'level')\n",
    "payment_df = payment_df.withColumn('level', payment_df.level.cast('int'))\n",
    "payment_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# did the user downgrade\n",
    "downgrade_event = udf(lambda x: 1 if x == 'Submit Downgrade' else 0, IntegerType())\n",
    "downgrade_df = df_clean.select(['userId','page'])\n",
    "downgrade_df = downgrade_df.withColumn('downgrade', downgrade_event('page'))\n",
    "downgrade_df = downgrade_df.withColumn('downgrade', max('downgrade').over(Window.partitionBy('userId')))\n",
    "downgrade_df = downgrade_df.select(['userId','downgrade']).dropDuplicates(['userId']) \n",
    "downgrade_df = downgrade_df.withColumn('downgrade', downgrade_df.downgrade.cast('int'))\n",
    "downgrade_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of songs the user listened to in total\n",
    "num_songs = df_clean.select('userID','song').groupBy('userID').count()\n",
    "num_songs.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of Thumbs-Up/Down\n",
    "thumbsup_event = udf(lambda x: 1 if x == 'Thumnbs Up' else 0, IntegerType())\n",
    "df_clean = df_clean.withColumn('thumbsUp', thumbsup_event('page'))\n",
    "num_thumbs_up = df_clean.select('userID','thumbsUp').groupBy('userID').mean().withColumnRenamed('count', 'num_thumbs_up') \n",
    "print(num_thumbs_up.show(5))\n",
    "num_thumbs_down = df_clean.select('userID','page').where(df_clean.page == 'Thumbs Down').groupBy('userID').mean().withColumnRenamed('count', 'num_thumbs_down') \n",
    "print(num_thumbs_down.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
